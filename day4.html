Why Innovators Should Study the Rise and Fall of the Venetian Empire

Most organizations would be happy to last for centuries, as the Venetian Republic did. From 697 to 1797 AD, Venice’s technological acumen, geographic position, and unconventionality were interlocking advantages that allowed the Most Serene Republic to flourish. But when change comes suddenly, it can turn strengths into weaknesses and sweep away even thousand-year success stories.

Venice’s military technology and the city’s pivotal location on the main trade routes of the time gave Venice several strong, mutually reinforcing advantages.

The Arsenal, an advanced naval munitions factory that anticipated by several centuries the production-line method of manufacture, was the beating heart of the Venetian naval industry. From the thirteenth century on, the Arsenal nurtured creativity and spurred innovation and entrepreneurship in the construction of its galleys.

The city’s geographic location helped it to defend itself from both land- and sea-based invaders. This location, consisting of a series of islands in a marshy lagoon, also pushed it to develop a (then unusual) trading and moneylending economy, since there was little land to support agriculture. And its position at the top of the Adriatic Sea allowed it to become a vital trading hub, connecting the East with the West via the Mediterranean.

If, as Michael Porter wrote, competitive advantage stems from how “activities fit and reinforce one another….creating a chain that is as strong as its strongest link,” then strategic fit is something that the Venetian Republic had in spades.

But, like a lot of successful entities, Venice reached a point where it focused more on exploitation than exploration: Venetian traders followed existing paths to success. Entrepreneurs chose not to move away from traditional pathways. Established practices and preferences became more popular than exploration and speculation. Merchants and traders played the game of incremental innovation by focusing on efficiency and optimization. Determined to grow their own fortunes rapidly, they pressed their feet to the accelerator rather than charting new courses.

But toward the end of the 16th century the world was changing in ways that would make Venice less relevant. The Arsenal’s focus on galley ships made sense when the Mediterranean was the most important trading waterway. Alessandro Barbero, professor of medieval history at the University of Eastern Piedmont, in Italy, notes that the galley remained for a long time the favorite vessel of Venetian navigators. But the invention of seafaring galleons allowed countries bordering the Atlantic to set up new trade routes that did not flow through the Adriatic.

This age of exploration triggered the beginning of Venice’s decline. One huge advance in technology — ships that could survive at sea for months, even years — weakened Venice’s competitive advantage and the strategic fit of its competencies.

The rise of the seafaring galleon meant Venice was suddenly disadvantaged by its location at the northern extremity of the Adriatic Sea. Moreover, its Arsenal was no longer at the cutting edge of naval technology. Venice’s economic importance had sharply contracted by the time Napoleon invaded, bringing the Venetian Empire to an official end.

What’s the lesson for entrepreneurs and innovators today? The stronger the assumption that the future will function as today does, the greater the gravitational force of the status quo. Organizations set in their ways slow down and never strive for new horizons. They are doomed to wither.

If you don’t want to be caught by surprise, you have to recognize that the future will be different from the past. The future is unfathomable, ambiguous, and open to every option. One major move by a competitor, or one new technology, is sometimes all it takes to end an empire. If your current business is like a carefully tended garden, with neat beds and high walls, that’s not enough. The next opportunity (or threat) may lie outside those walls, at the messy intersection of sectors and markets.

Entrepreneurs and innovators resist “success as usual” syndrome, exploring emerging technologies and new business models. They try to keep the big picture in mind and are wary of being too efficient and too optimized. This perspective helps them promote unconventional ways of thinking, solving problems, and challenging the status quo. They know the goal is not to chase a fixed horizon but to understand when and how the horizon moves as they approach it.





How Investors React When Companies Announce They’re Moving to a SaaS Business Model

On April 23, 2012, Adobe Inc. launched a Software-as-a-Service (SaaS) subscription version of its key product line, Creative Suite, causing its net income to plummet by almost 35% percent the following year. Yet by April 2016 Adobe’s stock price had nearly tripled from its value four years earlier. Adobe’s radical transformation from a product-based business model to a service-based one raised eyebrows in the industry, with many software vendors now wondering how radically they should approach the SaaS model.

Due to the fast growth of the SaaS market and the high valuations of SaaS startups, a move toward SaaS seems very compelling for traditional software vendors. For example, in 2014 IDC estimated that more than one-quarter of enterprise applications would be offered with the SaaS model by 2018, up from one-sixth in 2013. But the move to SaaS comes with considerable challenges: Firms will need to change their structure, sales culture, and incentives, and convince existing as well as new customers of the new offering’s value.

To understand how traditional software vendors can move to the SaaS model while managing the transformation’s challenges, we studied how the stock market reacted to publicly listed firms announcing they wanted to introduce a new SaaS offering from 2001 until the end of 2015. We distinguished (1) whether firms were introducing a completely new product or a product already existed and (2) whether the new product would be SaaS-only or a standard product “fallback” would exist. (Disclosure: One of us works at Amazon Web Services, a provider for many SaaS businesses.)

While a SaaS announcement on average neither decreases nor increases company value, we find big differences depending on the specifics of what’s announced. Investors preferred new SaaS products over product conversions and valued having a product fallback option. These choices may change the intra-day stock valuation by as much as 3.5% and 2.2%, respectively. In addition, we found that partnering with an external cloud service provider to deliver SaaS (instead of building and managing the cloud infrastructure on your own) leads to a further 2.9% increase in stock price on the announcement day.

Based on these results, we suggest decision makers follow three steps in order to manage the challenges of transforming towards a SaaS model:

Do not give up on the on-premise option yet.
Make sure existing products, processes, and culture do not prohibit the SaaS model from blossoming.
Collaborate and partner with cloud platform providers.


On the first point, of course, running two business models in parallel implies duplication of resources and can lead to the growth of the SaaS model being hindered. Software development, operations, and support have to be provided for both offerings, and because they differ from one another considerably, economies of scale cannot be reached in the way possible with just one business model. For example, Adobe’s finance team estimated that the cost of running both models side by side would cost them twice as much as simply offering one of the models. At the same time, sales personnel who are used to making large license-plus-maintenance deals are not likely to do well or be motivated to sell smaller monthly or yearly subscription packages.

Despite these challenges, our study found that investors increase their valuations of the software vendor’s stock by an average of 2.2% if the vendor makes clear in its announcement that the SaaS offering is provided in parallel to a perpetual licensing model. Even more surprising, this does not appear to apply only to existing products with an installed base of customers in place. Launching a new software product with both SaaS and perpetual licensing models is seen even more favorably by investors than launching the new product only with the SaaS model. There seems to be variety in customers’ requirements, meaning that software vendors would not be able to tap into the whole market without a perpetual license offering.

Consequently, software vendors should not look at Adobe’s example and draw a conclusion that the model of selling perpetual licenses to software is dead. Rather, they should consider finding a way to facilitate both business models — at least for a transition period. Two prime examples of software vendors that have patiently run two business models in parallel while transforming to the SaaS model are Autodesk and Splunk. Autodesk, a longtime market leader in computer aided design and engineering software, added a perpetual licensing option to all of its subscription offerings over a 15-year transition period before going all in with the SaaS model in 2016. Splunk, a leader in business intelligence founded in 2003, is still holding onto the perpetual licensing model, even for new product lines it has launched. The fact that Adobe, Autodesk, and Splunk have all been successful with dual approaches (both in terms of share price performance and growth of SaaS sales) shows that each vendor needs to understand its customers’ needs and gradually help them move to the SaaS model.

On the second point, software vendors should still make sure they embrace the SaaS model and give it the attention it needs to flourish. It’s easy for existing organizational structures and cultures to inhibit the growth of the new business model, which arguably has been a reason many major software vendors have been unable to establish leading SaaS offerings organically. Accordingly, our study found that the existence of a previous version of the product prior to the introduction of the SaaS offering reduces company value by an average of 3.5% compared to new product launches with the SaaS model. Thus software vendors should look to approach the SaaS business model by creating new product lines rather than converting their existing product portfolios, when possible. Dynatrace, a leader in application performance management software, has shown the importance of developing SaaS products from scratch, in isolation of existing product lines. It established an independent entity, Ruxit, in 2014 to develop a line of SaaS products in complete separation from Dynatrace’s established product lines and organization. The new business was given time and space to grow in isolation before being integrated into the Dynatrace portfolio two years later, in July 2016.

Finally, a big reason for the emergence of the SaaS model lies in the economies of scale and flexibility provided by dividing computing into independent components. Software vendors should see this as a great opportunity rather than a risk and should not try to build all the components themselves. For example, consider Zynga’s decision, announced in 2011, to build its own datacenters. Only four years later, in 2015, Zynga backtracked on the decision and started hosting its games on Amazon Web Services again. Our study found that announcements that implied the SaaS offering would be built in cooperation with cloud infrastructure and platform providers increased company valuations by an average of 2.9% as compared to other announcements, which underlines the importance of this finding.

In conclusion, decision makers at software firms can feel confident that launching new SaaS offerings will not be perceived negatively by their investors — but rather the opposite — provided that they follow the three steps we’ve outlined above.

































A Guide to Solving Social Problems with Machine Learning


It’s Sunday night. You’re the deputy mayor of a big city. You sit down to watch a movie and ask Netflix for help. (“Will I like Birdemic? Ishtar? Zoolander 2?”) The Netflix recommendation algorithm predicts what movie you’d like by mining data on millions of previous movie-watchers using sophisticated machine learning tools. And then the next day you go to work and every one of your agencies will make hiring decisions with little idea of which candidates would be good workers; community college students will be largely left to their own devices to decide which courses are too hard or too easy for them; and your social service system will implement a reactive rather than preventive approach to homelessness because they don’t believe it’s possible to forecast which families will wind up on the streets.

You’d love to move your city’s use of predictive analytics into the 21st century, or at least into the 20th century. But how? You just hired a pair of 24-year-old computer programmers to run your data science team. They’re great with data. But should they be the ones to decide which problems are amenable to these tools? Or to decide what success looks like? You’re also not reassured by the vendors the city interacts with. They’re always trying to up-sell you the very latest predictive tool. Decisions about how these tools are used seem too important for you to outsource, but raise a host of new issues that are difficult to understand.


This mix of enthusiasm and trepidation over the potential social impact of machine learning is not unique to local government or even to government: non-profits and social entrepreneurs share it as well.  The enthusiasm is well-placed. For the right type of problem, there are enormous gains to be made from using these tools. But so is the trepidation: as with all new “products,” there is potential for misuse. How can we maximize the benefits while minimizing the harm?

In applying these tools the last few years, we have focused on exactly this question. We have learned that some of the most important challenges fall within the cracks between the discipline that builds algorithms (computer science) and the disciplines that typically work on solving policy problems (such as economics and statistics). As a result, few of these key challenges are even on anyone’s radar screen. The good news is that many of these challenges, once recognized, are fairly straightforward to solve.

We have distilled what we have learned into a “buyer’s guide.” It is aimed at anyone who wants to use data science to create social good, but is unsure how to proceed.

How machine learning can improve public policy

First things first: There is always a new “new thing.” Especially in the social sector. Are these machine learning tools really worth paying attention to?

Yes. That’s what we’ve concluded from our own proof-of-concept project, applying machine learning to a dataset of over one million bond court cases (in joint work with Himabindu Lakkaraju and Jure Leskovec of Stanford University). Shortly after arrest, a judge has to decide: will the defendant await their legal fate at home? Or must they wait in jail? This is no small question. A typical jail stay is between two and three months. In making this life-changing decision, by law, the judge has to make a prediction: if released, will the defendant return for their court appearance, or will they skip court? And will they potentially commit further crimes?

We find that there is considerable room to improve on judges’ predictions.  Our estimates show that if we made pre-trial release decisions using our algorithm’s predictions of risk instead of relying on judge intuition, we could reduce crimes committed by released defendants by up to 25% without having to jail any additional people. Or, without increasing the crime rate at all, we could jail up to 42% fewer people. With 12 million people arrested every year in the U.S., this type of tool could let us reduce jail populations by up to several hundred thousand people.  And this sort of intervention is relatively cheap. Compared to investing millions (or billions) of dollars into more social programs or police, the cost of statistically analyzing administrative datasets that already exist is next-to-nothing. Plus, unlike many other proposals to improve society, machine learning tools are easily scaled.

By now, policymakers are used to hearing claims like this in sales pitches, and they should appropriately raise some skepticism. One reason it’s hard to be a good buyer of machine learning solutions is that there are so many overstated claims. It’s not that people are intentionally misstating the results from their algorithms. In fact, applying a known machine learning algorithm to a dataset is often the most straightforward part of these projects. The part that’s much more difficult, and the reason we struggled with our own bail project for several years, is accurately evaluating the potential impact of any new algorithm on policy outcomes. We hope the rest of this article, which draws on our own experience applying machine learning to policy problems, will help you better evaluate these sales pitches and make you a critical buyer as well.

Look for policy problems that hinge on prediction


Our bail experience suggests that thoughtful application of machine learning to policy can create very large gains. But sometimes these tools are sold like snake oil, as if they can solve every problem.

Machine learning excels at predicting things. It can inform decisions that hinge on a prediction, and where the thing to be predicted is clear and measurable.

For Netflix, the decision is what movie to watch. Netflix mines data on large numbers of users to try to figure out which people have prior viewing histories that are similar to yours, and then it recommends to you movies that these people have liked. For our application to pre-trial bail decisions, the algorithm tries to find past defendants who are like the one currently in court, and then uses the crime rates of these similar defendants as the basis for its prediction.

If a decision is being made that already depends on a prediction, why not help inform this decision with more accurate predictions? The law already requires bond court judges to make pre-trial release decisions based on their predictions of defendant risk. Decades of behavioral economics and social psychology teach us that people will have trouble making accurate predictions about this risk – because it requires things we’re not always good at, like thinking probabilistically, making attributions, and drawing inferences. The algorithm makes the same predictions judges are already making, but better.

But many social-sector decisions do not hinge on a prediction. Sometimes we are asking whether some new policy or program works – that is, questions that hinge on understanding the causal effect of something on the world. The way to answer those questions is not through machine learning prediction methods. We instead need tools for causation, like randomized experiments. In addition, just because something is predictable, that doesn’t mean we are comfortable having our decision depend on that prediction. For example we might reasonably be uncomfortable denying welfare to someone who was eligible at the time they applied just because we predict they have a high likelihood to fail to abide by the program’s job-search requirements or fail a drug test in the future.

Make sure you’re comfortable with the outcome you’re predicting

Algorithms are most helpful when applied to problems where there is not only a large history of past cases to learn from but also a clear outcome that can be measured, since measuring the outcome concretely is a necessary prerequisite to predicting. But a prediction algorithm, on its own, will focus relentlessly on predicting the outcome you provide as accurately as possible at the expense of everything else. This creates a danger: if you care about other outcomes too, they will be ignored. So even if the algorithm does well on the outcome you told it to focus on, it may do worse on the other outcomes you care about but didn’t tell it to predict.

This concern came up repeatedly in our own work on bail decisions. We trained our algorithms to predict the overall crime rate for the defendents eligible for bail. Such an algorithm treats every crime as equal. But what if judges (not unreasonably) put disproportionate weight on whether a defendant engages in a very serious violent crime like murder, rape, or robbery? It might look like the algorithm’s predictions leads to “better outcomes” when we look at overall rates of crime. But the algorithm’s release rule might actually be doing worse than the judges with respect to serious violent crimes specifically. The possibility of this happening doesn’t mean algorithms can’t still be useful. In bail, it turns out that different forms of crime are correlated enough so that an algorithm trained on just one type of crime winds up out-predicting judges on almost every measure of criminality we could construct, including violent crime. The point is that the outcome you select for your algorithm will define it. So you need to think carefully about what that outcome is and what else it might be leaving out.

Check for bias

Another serious example of this principle is the role of race in algorithms. There is the possibility that any new system for making predictions and decisions might exacerbate racial disparities, especially in policy domains like criminal justice. Caution is merited: the underlying data used to train an algorithm may be biased, reflecting a history of discrimination. And data scientists may sometimes inadvertently report misleading performance measures for their algorithms. We should take seriously the concern about whether algorithms might perpetuate disadvantage, no matter what the other benefits.

Ultimately, though, this is an empirical question. In our bail project, we found that the algorithm can actually reduce race disparities in the jail population. In other words, we can reduce crime, jail populations and racial bias – all at the same time – with the help of algorithms.

This is not some lucky happenstance. An appropriate first benchmark for evaluating the effect of using algorithms is the existing system – the predictions and decisions already being made by humans. In the case of bail, we know from decades of research that those human predictions can be biased. Algorithms have a form of neutrality that the human mind struggles to obtain, at least within their narrow area of focus. It is entirely possible—as we saw—for algorithms to serve as a force for equity. We ought to pair our caution with hope.

The lesson here is that if the ultimate outcome you care about is hard to measure, or involves a hard-to-define combination of outcomes, then the problem is probably not a good fit for machine learning. Consider a problem that looks like bail: Sentencing. Like bail, sentencing of people who have been found guilty depends partly on recidivism risk. But sentencing also depends on things like society’s sense of retribution, mercy, and redemption, which cannot be directly measured. We intentionally focused our work on bail rather than sentencing  because it represents a point in the criminal justice system where the law explicitly asks narrowly for a prediction. Even if there is a measurable single outcome, you’ll want to think about the other important factors that aren’t encapsulated in that outcome – like we did with race in the case of bail – and work with your data scientists to create a plan to test your algorithm for potential bias along those dimensions.


Verify your algorithm in an experiment on data it hasn’t seen

Once we have selected the right outcome, a final potential pitfall stems from how we measure success. For machine learning to be useful for policy, it must accurately predict “out-of-sample.” That means it should be trained on one set of data, then tested on a dataset it hasn’t seen before. So when you give data to a vendor to build a tool, withhold a subset of it. Then when the vendor comes back with a finished algorithm, you can perform an independent test using your “hold out” sample.

An even more fundamental problem is that current approaches in the field typically focus on performance measures that, for many applications, are inherently flawed. Current practice is to report how well one’s algorithm predicts only among those cases where we can observe the outcome. In the bail application this means our algorithm can only use data on those defendants who were released by the judges, because we only have a label providing the correct answer to whether the defendant commits a crime or not for defendants judges chose to release. What about defendants that judges chose not to release? The available data cannot tell us whether they would have reoffended or not.

This makes it hard to evaluate whether any new machine learning tool can actually improve outcomes relative to the existing decision-making system — in this case, judges. If some new machine learning-based release rule wants to release someone the judges jailed, we can’t observe their “label”, so how do we know what would happen if we actually released them?

This is not merely a problem of academic interest. Imagine that judges have access to information about defendants that the algorithm does not, such as whether family members show up at court to support them. To take a simplified, extreme example, suppose the judge is particularly accurate in using this extra information and can apply it to perfectly predict whether young defendants re-offend or not. Therefore the judges release only those young people who are at zero risk for re-offending. The algorithm only gets to see the data for those young people who got released – the ones who never re-offend. Such an algorithm would essentially conclude that the judge is making a serious mistake in jailing so many youthful defendants (since none of the ones in its dataset go on to commit crimes). The algorithm would recommend that we release far more youthful defendants. The algorithm would be wrong. It could inadvertently make the world worse off as a result.

In short, the fact that an algorithm predicts well on the part of the test data where we can observe labels doesn’t necessarily mean it will make good predictions in the real world. The best way to solve this problem is to do a randomized controlled trial of the sort that is common in medicine. Then we could directly compare whether bail decisions made using machine learning lead to better outcomes than those made on comparable cases using the current system of judicial decision-making. But even before we reach that stage, we need to make sure the tool is promising enough to ethically justify testing it in the field. In our bail case, much of the effort went into finding a “natural experiment” to evaluate the tool.

Our natural experiment built on two insights. First, within jurisdictional boundaries, it’s essentially random which judges hear which cases. Second, judges are quite different in how lenient they are. This lets us measure how good judges are at selecting additional defendants to jail. How much crime reduction does a judge with a 70% release rate produce compared to a judge with an 80% release rate? We can also use these data to ask how good an algorithm would be at selecting additional defendants to jail. If we took the caseload of an 80% release rate judge and used our algorithm to pick an additional 10% of defendants to jail, would we be able to achieve a lower crime rate than what the 70% release rate judge gets? That “human versus machine” comparison doesn’t get tripped up by missing labels for defendants the judges jailed but the algorithm wants to release, because we are only asking the algorithm to recommend additional detentions (not releases).  It’s a comparison that relies only on labels we already have in the data, and it confirms that the algorithm’s predictions do indeed lead to better outcomes than those of the judges.

It can be misguided, and sometimes outright harmful, to adopt and scale up new predictive tools when they’ve only been evaluated on cases from historical data with labels, rather than evaluated based on their effect on the key policy decision of interest. Smart users might go so far as to refuse to use any prediction tool that does not take this evaluation challenge more seriously.

Remember there’s still a lot we don’t know

While machine learning is now widely used in commercial applications, using these tools to solve policy problems is relatively new. There is still a great deal that we don’t yet know but will need to figure out moving forward.

Perhaps the most important example of this is how to combine human judgment and algorithmic judgment to make the best possible policy decisions. In the domain of policy, it is hard to imagine moving to a world in which the algorithms actually make the decisions; we expect that they will instead be used as decision aids.

For algorithms to add value, we need people to actually use them; that is, to pay attention to them in at least some cases. It is often claimed that in order for people to be willing to use an algorithm, they need to be able to really understand how it works. Maybe. But how many of us know how our cars work, or our iPhones, or pace-makers? How many of us would trade performance for understandability in our own lives by, say, giving up our current automobile with its mystifying internal combustion engine for Fred Flintstone’s car?

The flip side is that policymakers need to know when they should override the algorithm. For people to know when to override, they need to understand their comparative advantage over the algorithm – and vice versa. The algorithm can look at millions of cases from the past and tell us what happens, on average. But often it’s only the human who can see the extenuating circumstance in a given case, since it may be based on factors not captured in the data on which the algorithm was trained. As with any new task, people will be bad at this in the beginning. While they should get better over time, there would be great social value in understanding more about how to accelerate this learning curve.

Pair caution with hope

A time traveler going back to the dawn of the 20th century would arrive with dire warnings. One invention was about to do a great deal of harm. It would become one of the biggest causes of death—and for some age groups the biggest cause of death. It would exacerbate inequalities, because those who could afford it would be able to access more jobs and live more comfortably. It would change the face of the planet we live on, affecting the physical landscape, polluting the environment and contributing to climate change.

The time traveler does not want these warnings to create a hasty panic that completely prevents the development of automobile transportation. Instead, she wants these warnings to help people skip ahead a few steps and follow a safer path: to focus on inventions that make cars less dangerous, to build cities that allow for easy public transport, and to focus on low emissions vehicles.

A time traveler from the future talking to us today may arrive with similar warnings about machine learning and encourage a similar approach. She might encourage the spread of machine learning to help solve the most challenging social problems in order to improve the lives of many. She would also remind us to be mindful, and to wear our seatbelts.










Did Trade with China Make U.S. Manufacturing Less Innovative?

In early 2016 economists David Autor, David Dorn, and Gordon Hanson published an influential paper that highlighted some of the costs of global trade. They reviewed the literature and reported that trade with China had cost the U.S. as many as one million manufacturing jobs since 1999, had lowered wages, and had not led to the new jobs and industries that trade proponents had promised.

The main case for trade, though, was always that it would improve overall welfare by allowing a greater variety of products to be produced more efficiently. China might focus on producing labor-intensive goods, but the U.S. would shift toward work that was more valuable and innovative. If trade’s “winners” compensated the “losers,” everyone could benefit.

On Monday the same trio of economists published a paper, with coauthors Pian Shu and Gary Pisano, that complicates this story. Was increased trade with China really pushing U.S. companies to become more innovative? For manufacturers, at least, they found that the answer was no. In fact, the relationship went in the opposite direction: U.S. manufacturers exposed to competition from Chinese imports became far less innovative.

The researchers looked at trade data from 1991 to 2007, during which time China became a global manufacturing powerhouse. They measured how varying levels of exposure to Chinese imports affected U.S. manufacturers’ performance and patenting. Competition with China was associated with decreased performance on several measures. “Aggregate firm sales revenues, employment, available capital, market valuation, and investments in new technology have diminished as competitive conditions have tightened, thereby contributing to diminished profitability,” the authors wrote. These effects held up after controlling for numerous other variables.

The first takeaway from this paper is that more competition, from trade or otherwise, doesn’t necessarily lead to more innovation. While competition can force firms to innovate to fend off rivals, it can also cut profit margins, leaving companies with less to invest in research and development. A widely cited paper from 2002 posited an inverted-U relationship between competition and innovation: Too little competition, and firms won’t bother to innovate; too much, and they won’t be able to afford to.

The second takeaway, which is trickier, concerns the relationship between trade and innovation. Is it possible that trade has hurt rather than bolstered American innovation? The research literature is mixed. There’s evidence that European manufacturers became more innovative in response to competition from China, for example, but also that Canadian manufacturers became less innovative. It’s possible that this difference corresponds to the inverted-U. Because manufacturing prior to China’s rise was more competitive in the U.S. than in the EU, trade with China could have pushed the U.S. onto the too-much-competition side of things while spurring European manufacturers to become more innovative.

And the new U.S. evidence is only looking at innovation in manufacturing. It’s possible that other U.S. industries became more innovative, thanks to cheaper inputs made possible by trade. “I have no reason to think that globalization reduces innovation in net [worldwide],” Autor told me, but he emphasized that manufacturing plays an outsize role in U.S. innovation. As he and his coauthors write in the paper, “Manufacturing still generates more than two-thirds of U.S. R&D spending and U.S. corporate patents despite accounting for less than one-tenth of U.S. private non-farm employment.”

It’s hard to say exactly what this new evidence means for the overall case for global trade, and the authors caution against overgeneralizing the result. Most economists believe that trade is beneficial in the long run, but long-run prosperity depends heavily on innovation. If this new paper shows anything, it’s that we can’t just assume that more competition and open markets will deliver new ideas. Under the right circumstances, they can. But if they squeeze entire industries’ ability to invest or make it easy to rely on cheap labor rather than technology, the result could be less innovation, rather than more.






Google’s antitrust battle with the European Union seems to be heating up. In recent weeks, the company has rebutted the European Commission’s charges that it uses its internet search engine to give its shopping services an unfair advantage over rivals, improperly uses its AdSense ad-placement service to restrict third-party websites from displaying search advertisements from Google’s competitors, and unfairly exploits the dominant position of its Android operating system with smartphone manufacturers and mobile network operators (see its November 3 and November 10 blog posts.)

We think Google’s rebuttal of the charges against its shopping and ad-placement services are effective, and it has even made some changes to its products to address issues raised by the European Union. But we think it is unlikely that Google will be able to prevail in the Android case unless it abandons its contention that E.U. authorities should adopt a U.S. perspective. Instead, it should try to persuade them that competition in the mobile space is radically different from that in traditional markets and consequently, the European Union — and the United States, for that matter — should revamp their antitrust laws.

Companies like Google operate simultaneously in multiple ecosystems, as do their competitors. As such, any analysis of competitive dynamics must be done across ecosystems — something the European Union hasn’t done. Instead, it has just focused on the mobile-operating-system market. Given that Google has a European market share of more than 90% in general internet search services, licensable smart mobile operating systems and app stores for the Android mobile operating system, E.U. antitrust authorities seem to believe that Google needs to be policed from stifling innovation in the marketplace.

In its response to the European Union, Google argues that its Android operating system has made it cheaper and faster for device manufacturers to bring devices to market, lowered prices for smartphones, and increased the distribution opportunities for apps. In other words, Google bases its reply on the prevailing perspective in the United States with respect to antitrust enforcement.

The Mobile Industry Stack

To understand the issues involved in this tussle, it is important to understand that firms like Google, Apple, Amazon, and Facebook are simultaneously competing and cooperating in multiple fronts — a phenomenon we call interlocking ecosystems. Since mobile devices play an important role in making it possible for these interlocking ecosystems to work smoothly, it is critical to understand the mobile industry stack.




The typical stack is comprised of layers of functionality, each of which delivers a unique value proposition for customers. Every layer of the stack depends on the layer below to deliver its promised function. Since the layers communicate using application program interfaces (APIs) provided by vendors operating in the layer below, those vendors that dominate one layer can use their power to control how the layer above develops. In a notorious instance, Microsoft reportedly hoped to “cut off Netscape’s air supply” by threatening to withhold crucial technical support from Intel. When a company uses its own products for each layer (e.g., Apple), the industry stack is vertically integrated. When different suppliers provide the required functionality for each layer, the leadership of the stack is divided.

Android is Google’s open-source operating system (OS), which means that it can be adopted by any device manufacturer and modified to provide different functionality. As a result, 90% of device manufacturers in Europe use Android as their OS. Some modify the OS; in those cases, it is called an Android fork.

The next layer of the stack, the application layer, is the most important for the majority of today’s consumers. People pay for, download, and use third-party applications via the Google Play Store. One of the most popular apps is Google Search, which is also the primary source of Google’s advertising revenue and therefore is vital to Google’s business model.

Google requires mobile-device manufacturers to sign an “anti-fragmentation agreement,” which states that manufacturers that want to pre-install Google proprietary apps (including Google Play Store and Google Search) on its devices cannot include Android forks on those same devices. Consequently, device manufacturers that adopt the Google stack must bundle both Google Search and Google Play, which essentially locks out the other search engine providers and store fronts. In other words, while Google supports divided technical leadership for part of the stack, it tightly controls the applications layer.

From a stack perspective, Google provides free functionality at a lower level of the stack and makes up for it by charging for the higher-level functionality. To subsidize the free Android OS, Google has ensured that consumers provide data and exclusively use its search application (supported by paid advertising) in the higher layer.


From Google’s perspective, this approach has the benefit of increasing efficiency in the mobile industry stack. Google has at once freed device manufacturers from having to invest in developing complex operating systems, allowed them to focus on their core competence of designing and manufacturing of devices, and thereby lower their costs. As Kent Walker, Google’s senior vice president and general counsel, argues, Android has made it cheaper and faster for device makers to bring new devices to market. At the same time, Google suggests, it has enabled consumers to benefit immensely by falling device prices, access to a variety of devices, and ease of use of thousands of apps.

Competition Policy: U.S. vs. E.U.

Google’s argument relies heavily on the prevailing American perspective of competition policy. In the United States, legal precedents, as well as enforcement efforts, approach competition policy with consumer welfare as its lodestar. This approach argues that promoting efficiency with which firms operate leads to enhanced consumer welfare since improved efficiency leads to lower costs which in turn leads to lower prices. In that context, preserving competition in the marketplace is only a means to an end. As one observer puts it, “efficiencies are the goal; competition is the process.” By this logic, Google’s approach to the mobile stack is efficiency enhancing whereby consumers benefit.

Unfortunately for Google, the E.U. antitrust enforcers have a slightly different perspective. They, too, start off with consumer welfare as the long-run goal, but they insist that moves by rivals that could be efficiency enhancing in the short run could be detrimental in the long run, especially if those moves could thwart technological innovation and competition. In other words, E.U. authorities put more stress on what one might call dynamic efficiency in markets.

The differing U.S. and E.U. antitrust perspectives can lead to different outcomes when confronted with the same set of facts. For example, in the case of the proposed merger between GE and Honeywell, U.S. antitrust authorities gave it a go-ahead based on projected cost efficiencies whereas the E.U. authorities denied it since it has the potential to lower competition in the long run.

In the case of mobile operating system, the European Union makes three arguments, all of which have the notion of dynamic efficiency at their core.

One, the European Union argues that since “Google has made the licensing of the Play Store on Android devices conditional on Google Search being pre-installed and set as default search service,” rival search engines are not able to get a foothold in the devices sold in the European Union. Moreover, it has also reduced the incentives of manufacturers to pre-install competing search apps, as well as the incentives of consumers to download such apps. This is important because the European Union’s research has shown that consumers rarely change or delete pre-installed apps (unless the pre-installed app is of particularly poor quality). In other words, Google’s approach reduces the likelihood of future competition in search.

Two, the European Union argues that Google’s “anti-fragmentation agreement” also has the effect of reducing dynamic efficiency. It says: “Google’s conduct prevented manufacturers from selling smart mobile devices based on a competing Android fork, which had the potential of becoming a credible alternative to the Google Android operating system. In doing so, Google has also closed off an important way for its competitors to introduce apps and services, in particular general search services, which could be pre-installed on Android forks.”

Three, in the European Union’s view, Google reduces dynamic efficiency through its moves to grant significant financial incentives to some of the largest smartphone and tablet makers as well as mobile network operators to persuade them to exclusively pre-install Google Search on their devices. Again, this has the effect of denying competing search devices a toehold in the market.

Incidentally, none of these arguments is novel or without merit. The European Union has before it instances where similar issues played out to the detriment of dynamic efficiency in other markets. For instance, in its dealings with PC manufacturers in the early 1980s, Microsoft used a seemingly innocuous pricing approach to ensure that manufacturers had no incentives to install a rival operating system. This led to Microsoft achieving a near-monopoly position that has lasted to this day. Moreover, Google itself has demonstrated how keen it is to preserve its ability to collect data through its search engine. It has used its dominant position to ban the privacy app called Disconnect from its Play Store since the app can shield users against invisible tracking tools. Through its anti-fragmentation agreement, Google can probably ensure that the privacy app does not get any traction in the market.

How Google Should Respond

Based on Microsoft’s experience with European Union in the context of its browser and more recent decisions that the European Union has made with respect to other firms in the context of mergers and acquisitions, it is unlikely that Google will prevail in the E.U case if its persists with its present arguments.  Google, however, could argue that one of the E.U. assumptions about competition is flawed.

E.U. authorities claim that Android is dominating the mobile OS market. While it is true that Google is dominating the OS market, OS is only one part of several ecosystems in the digital universe. For example, in the home-automation market, Amazon, Apple, Google, and Microsoft are all competing for market share. The OS layer is only a small part of the overall solution. The conversational layer (functional layer of the stack that allows consumers to talk to machines and execute commands) is currently dominated by Amazon and Apple. There are thousands of such ecosystems, and Google is certainly not dominating all of them.


Google is one of the digital titans, a group that includes, Alibaba, Amazon, Apple, Baidu, Facebook, and Tencent. These companies use their digital superiority to participate in countless ecosystems by investing in R&D, developing partnerships, or by using APIs in shrewd and unexpected ways. In doing so, they may dominate one market and, at the same time, be a new entrant in a different market. They apply their data assets, algorithms, third-party developers, and managerial and engineering talent across multiple ecosystems.

We live in a world where companies compete by being part of an ecosystem, a world where the winners can change at any time because there are always newer, shinier, and more innovative solutions in the works. Regulators are making a mistake by simply looking at one ecosystem and declaring Google the winner. That is like declaring a Super Bowl winner based on the team that scored the most points in the first quarter of the season opener! This approach doesn’t account for adjustments and improvements made by the opposing team or by future opponents.

Note how this argument takes EU’s dynamic efficiency concerns and turns them around. The key question to ask is this: Do we seek to preserve dynamic efficiency at each ecosystem separately or do we seek to preserve it at the intersection of multiple ecosystems? There are no easy answers to this question. Still, it will have the effect of making the E.U. competition authorities to think more deeply and give Google an out.

















What Cancer Researchers Can Learn from Direct-to-Consumer Companies


Organizations striving to find new ways to attack cancer have much to learn from direct-to-consumer (DTC) companies. Specifically, they can profit from DTC firms’ expertise in persuading their customers to provide and share their data. This is something many cancer patients don’t do because they are unaware of the data’s importance or their power to instruct institutions to share it.

Advances in genome-sequencing technologies and powerful analytics are increasingly being used to pinpoint errant genes and other molecular abnormalities that drive cancer’s growth. This knowledge can be used to help doctor match some patients — for example, lung cancer patients with mutations in the ALK or EGFR genes or breast cancer patients’ tumors that overexpress the HER2 protein — to treatments that target the unique aspects of their cancer. This personalized approach could one day help all people with cancer benefit from smart medicines, but only if patients opt to share their sequencing data and other health information and insist that health care organizations comply with their wishes.

That’s because researchers must deeply analyze a massive amount of patient data, starting with patients’ sequencing data, to pinpoint cancer-causing mutations. Access to a massive amount of patient data is key because it ensures two things: first, that abnormalities that occur only sporadically aren’t overlooked, and second, that random abnormalities that occur by chance are spotted as false discoveries. These insights — again, made possible only by learning from as many patients as possible — inform our collective knowledge of cancer and effectively predict how to best treat it in the individual patient.

Unfortunately, far too many patients remain hesitant to share their health data, even though they routinely share all sorts of personal data when buying consumer products online. The reasons for this, of course, are many. Some have privacy concerns. Some are not asked to share their data or don’t know how to do so. We suspect that many others just don’t understand the value of their data; they are unaware of the powerful role their data plays in advancing research breakthroughs, let alone that sharing it may give them their best shot of beating the disease.

What can the field of precision oncology learn from DTC and direct-to patient (DTP) companies that have amassed a treasure trove of personal data? To find out, the HBS Kraft Precision Medicine Accelerator, which we cochair, recently brought together leaders from some of the most innovative DTC companies: Under Armour, Rent the Runway, Wayfair, Peloton, and Uber, to name a few.

Each company collects vast amounts of customers’ personal data, from their browsing history to their recent purchases. Using sophisticated computer algorithms, each then turns this data into insights about future behaviors, from what running shoes an individual will want to make her a better athlete to what dress she just has to have for that special occasion. This creates value for consumers — by saving them money, saving them time in making choices, or by informing their decisions — and allows the companies to retain consumers over the long haul and amass even more data.

What can the field of precision oncology learn from this DTC approach?

Know the patient. Organizations collecting patient data — including academic medical centers, foundations that sponsor cancer research, and pharmaceutical companies — must first identify the needs, pains, hopes, expectations, and behaviors of each person living with cancer. Where do they get their health information? How well do they understand their personal health data? Do they access it? Do they share it? If so, how? Answering these questions will allow organizations involved in cancer research to meet patients where they are, just as DTC companies do.

Tell a great story. The best brands use storytelling to amplify the company’s manifesto and mission. Under Armour is not just selling hoodies; it’s building better athletes. Rent the Runway is not just dressing the world’s women; it’s building self-confidence. People in the health care field often talk in industry jargon and drop buzzwords like “sequencing” or “biomarkers” that just don’t resonate with real people with cancer. They must use heartfelt storytelling, with emotional arcs, to connect with patients in a meaningful way.

Use social media in an effective way. DTC companies are pros at using social media to connect with consumers. The precision oncology space would greatly benefit from adopting a similar approach by reaching patients and caregivers, particularly those identified as influences or ambassadors, on multiple social media platforms where they get most of their cancer-related information.

Start small to go big. An email address is the beginning of a new relationship. If members of the health care community collect this and other data — a “follow,” a “like” — as DTC companies have long done, they will be well on their way to engaging patients in their first conversation about data sharing. It will take time to build trust, of course. But without collecting basic contact information, you can’t get started.

Create value. A number of DTC companies have created membership programs that reward consumers for their continued loyalty with something of value to them: discounts, freebies, and so on. To encourage patients to share their data and keep them engaged, health care organizations can offer similar incentives — for example, alerting patients to new clinical trials of medicines that target their specific mutations, or even something as simple as an exclusive hat or T-shirt.

The expectations and the motivations of people with cancer aren’t all that different from those of other consumers. By applying the knowledge and best practices from DTC companies, organizations that want to encourage the collecting and sharing of personal health data can be more effective in enlisting patients in the cause. And that, in turn, will make a major difference in finding new treatments and cures for cancer.
